"""
ë ˆì´ì‹± ê²Œì„ ì—ì´ì „íŠ¸ - Headless í•™ìŠµ ë²„ì „ (ë Œë”ë§ ì—†ì´ ë¹ ë¥¸ í•™ìŠµ)
Action ë‹¨ìˆœí™” ë²„ì „: 6ê°œ ì•¡ì…˜ìœ¼ë¡œ friction ê¸°ë°˜ ì œì–´
[ìˆ˜ì •ë¨] Frame skipping ë°©ì‹ ê°œì„  - action ë‹¨ìœ„ë¡œ 1ê°œì˜ transitionë§Œ ì €ì¥
[ìˆ˜ì •ë¨] ëª¨ë¸ ì €ì¥ ê¸°ì¤€ - 10íšŒ í…ŒìŠ¤íŠ¸ 100% ì„±ê³µ + í‰ê·  ì‹œê°„ ë¹„êµ
"""

import pygame
import random
import math
import numpy as np
from racing_game_2d import RacingGame 
import torch
import torch.nn as nn
import copy
import time
import os

class DQN:
    def __init__(self, input_size=28, output_size=6, replay_memory_length=100000, num_segments=5, lr=0.0001, layer_num=3, max_size=512):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.output_size = output_size
        self.layer_num = layer_num
        self.max_size = max_size
        
        self.model = self.create_model(input_size, 
                                       output_size, 
                                       self.layer_num, 
                                       self.max_size
                                       ).to(self.device)
        
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)
        self.loss_fn = nn.MSELoss()
        
        self.gamma = 0.99
        self.epsilon_min = 0.01
        
        self.segment_epsilon = {i: 1.0 for i in range(num_segments)}
        self.segment_decay = 0.995
        self.learned_epsilon = 0.1
        self.epsilon = 1.0
        
        self.replay_memory = []
        self.replay_memory_length = replay_memory_length
        
        self.action_repeat_map = {
            0: 5,   # ì§ì§„
            1: 5,   # ì§ì§„+ì¢Œ
            2: 5,   # ì§ì§„+ìš°
            3: 5,  # ì§ì§„+ë¸Œë ˆì´í¬+ì¢Œ (ë“œë¦¬í”„íŠ¸)
            4: 5,  # ì§ì§„+ë¸Œë ˆì´í¬+ìš° (ë“œë¦¬í”„íŠ¸)
            5: 2,   # ë¸Œë ˆì´í¬
        }
    
    # ëª¨ë¸ ìƒì„± (DQN ì´ë¯€ë¡œ ë‹¨ìˆœí•œ layer ë‚˜ì—´)    
    def create_model(self, input_size, output_size, layer_num, max_size):
        layers = []
        
        layers.append(nn.Linear(input_size, max_size))
        layers.append(nn.ReLU())
        
        current_size = max_size
        for i in range(layer_num - 1):
            next_size = current_size // 2
            layers.append(nn.Linear(current_size, next_size))
            layers.append(nn.ReLU())
            current_size = next_size
        
        layers.append(nn.Linear(current_size, output_size))
        
        return nn.Sequential(*layers)
    
    # Epsilon ê°’ ì—…ë°ì´íŠ¸
    def update_epsilon(self, current_segment, learned_until):
        if current_segment <= learned_until:
            self.epsilon = self.learned_epsilon
        else:
            self.epsilon = self.segment_epsilon.get(current_segment, 1.0)
    
    def decay_segment_epsilon(self, segment):
        if segment in self.segment_epsilon:
            self.segment_epsilon[segment] = max(
                self.segment_epsilon[segment] * self.segment_decay,
                self.epsilon_min
            )
    
    def get_real_action(self, action_index): 
        # 6ê°œ ì•¡ì…˜ + ê³ ì •ëœ frame ìˆ˜ ë§¤í•‘
        actions = {
            # ì§ì§„
            0: {'forward': True, 'backward': False, 'left': False, 'right': False, 'brake': False},
            # ì§ì§„ + ì¢Œ
            1: {'forward': True, 'backward': False, 'left': True, 'right': False, 'brake': False},
            # ì§ì§„ + ìš°
            2: {'forward': True, 'backward': False, 'left': False, 'right': True, 'brake': False},
            # ì§ì§„ + ë¸Œë ˆì´í¬ + ì¢Œ (ë“œë¦¬í”„íŠ¸)
            3: {'forward': True, 'backward': False, 'left': True, 'right': False, 'brake': True},
            # ì§ì§„ + ë¸Œë ˆì´í¬ + ìš° (ë“œë¦¬í”„íŠ¸)
            4: {'forward': True, 'backward': False, 'left': False, 'right': True, 'brake': True},
            # ë¸Œë ˆì´í¬
            5: {'forward': False, 'backward': False, 'left': False, 'right': False, 'brake': True},
        }
        return actions.get(action_index, actions[0])
    
    def get_action_frames(self, action_index):
        """ê° ì•¡ì…˜ì— í•´ë‹¹í•˜ëŠ” í”„ë ˆì„ ìˆ˜ ë°˜í™˜"""
        return self.action_repeat_map.get(action_index, 3)
    
    def predict(self, state, greedy=False):
        """ì•¡ì…˜ ì˜ˆì¸¡ (epsilon-greedy ë˜ëŠ” greedy)"""
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            q_values = self.model(state_tensor)
            if not greedy and random.random() < self.epsilon: 
                return q_values, random.randint(0, self.output_size - 1)
            else: 
                return q_values, q_values.argmax().item()
    
    def get_frame_reward(self, state, coll, goal, curr_distance, curr_time, max_time, 
                         cp_r, dis_gap, action_index, max_speed, prev_distance): 
        """
        ë‹¨ì¼ í”„ë ˆì„ì— ëŒ€í•œ ë³´ìƒ ê³„ì‚°
        - ì¶©ëŒ/ê³¨ ë„ë‹¬ ë“± ì´ë²¤íŠ¸ ë³´ìƒ
        - ì§„í–‰ ê±°ë¦¬ ë³´ìƒ
        - ì†ë„ ë³´ìƒ ë° ì €ì† íŒ¨ë„í‹°
        - ë“œë¦¬í”„íŠ¸ ë³´ë„ˆìŠ¤ (ê³ ì†ì¼ ë•Œë§Œ)
        - ì²´í¬í¬ì¸íŠ¸ ë³´ìƒ (ì†ë„ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ì ìš©)
        """
        speed_norm = state[14]
        is_drifting = state[19] > 0.5
        is_turning = action_index in [0, 1, 2]
        is_brake_only = action_index == 5
        
        # ê¸°ë³¸ ì´ë²¤íŠ¸ ë³´ìƒ
        if coll: 
            r = -200
        elif goal: 
            time_ratio = curr_time / (max_time * 1000)
            r = 100 + (1 - time_ratio) ** 2 * 500.0
        else: 
            r = 0
        
        # ì§„í–‰ ë³´ìƒ: ì´ì „ í”„ë ˆì„ ëŒ€ë¹„ ëª©í‘œ ì²´í¬í¬ì¸íŠ¸ê¹Œì§€ ê±°ë¦¬ ê°ì†ŒëŸ‰
        r_dis = (prev_distance - curr_distance) * 0.1
        
        # ì‹œê°„ íŒ¨ë„í‹°: ë§¤ í”„ë ˆì„ë§ˆë‹¤ ì†ŒëŸ‰ì˜ íŒ¨ë„í‹°
        r_time = 0.01
        
        # ì†ë„ ë³´ìƒ: ì •ê·œí™”ëœ ì†ë„ì— ë¹„ë¡€
        r_speed = speed_norm * 0.3
        
        # ì €ì† íŒ¨ë„í‹°: ë„ˆë¬´ ëŠë¦¬ê²Œ ì£¼í–‰ ì‹œ ì¶”ê°€ íŒ¨ë„í‹°
        if speed_norm < 0.15:
            r_speed -= 0.5
        
        # ë“œë¦¬í”„íŠ¸ ë³´ë„ˆìŠ¤: ê³ ì†ì—ì„œ ë“œë¦¬í”„íŠ¸ ì‹œ ë³´ë„ˆìŠ¤, ì €ì†ì—ì„œëŠ” íŒ¨ë„í‹°
        drift_bonus = 0
        if is_drifting:
            if speed_norm > 0.4:
                drift_bonus = speed_norm * 1.0
            else:
                drift_bonus = -0.2
        
        # ë¸Œë ˆì´í¬ë§Œ ë°Ÿê³  ì €ì†ì¼ ë•Œ íŒ¨ë„í‹°
        if is_brake_only and speed_norm < 0.15:
            r_speed -= 0.5
        
        # ì²´í¬í¬ì¸íŠ¸ ë³´ìƒ: ì†ë„ì™€ ë“œë¦¬í”„íŠ¸ ìƒíƒœì— ë”°ë¼ ê°€ì¤‘ì¹˜ ì ìš©
        if cp_r > 0:
            if is_drifting and speed_norm > 0.4:
                cp_r *= 3.0
            elif speed_norm > 0.3:
                cp_r *= 2.0
            elif speed_norm < 0.15:
                cp_r *= 0.3
        
        reward = r + r_dis + r_speed - r_time + cp_r + drift_bonus
        return reward
    
    def add_memory(self, memory): 
        if len(self.replay_memory) < self.replay_memory_length: 
            self.replay_memory.append(memory)
        else:
            self.replay_memory.pop(0)
            self.replay_memory.append(memory)
    
    def train_step(self, batch_size, target_net):
        """
        DQN í•™ìŠµ ìŠ¤í…
        - Standard DQN ë°©ì‹ìœ¼ë¡œ í•™ìŠµ
        - Target networkë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì •ì ì¸ í•™ìŠµ
        """
        # Replay memoryì—ì„œ ëœë¤ ìƒ˜í”Œë§
        batch = random.sample(self.replay_memory, batch_size)
        
        # Memory format: [state, action, reward, next_state, done]
        states = torch.tensor([m[0] for m in batch], dtype=torch.float32).to(self.device)
        actions = torch.tensor([m[1] for m in batch], dtype=torch.int64).to(self.device)
        rewards = torch.tensor([m[2] for m in batch], dtype=torch.float32).to(self.device)
        next_states = torch.tensor([m[3] for m in batch], dtype=torch.float32).to(self.device)
        dones = torch.tensor([m[4] for m in batch], dtype=torch.float32).to(self.device)
        
        # í˜„ì¬ ìƒíƒœì—ì„œì˜ Qê°’ ê³„ì‚°
        Q_values = self.model(states)
        Q_currents = Q_values.gather(1, actions.unsqueeze(1)).squeeze(1)
        
        # Target Qê°’ ê³„ì‚°
        with torch.no_grad():
            next_Q = target_net.model(next_states)
            max_next_Q = torch.max(next_Q, dim=1)[0]
            # Bellman equationìœ¼ë¡œ target ê³„ì‚°
            Q_targets = rewards + self.gamma * max_next_Q * (1 - dones)
        
        # Loss ê³„ì‚°
        loss = self.loss_fn(Q_currents, Q_targets)
        
        # ì—­ì „íŒŒ ë° íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
    
        return loss.item()


def get_sensors(game):
    sensors = []
    sensor_range = game.car.sensor_range
    direction_num = 12
    # 12 ë°©í–¥ì— í•´ë‹¹í•˜ëŠ” ê±°ë¦¬ ê³„ì‚°. (sensor_range ë§Œí¼)
    for i in range(direction_num):
        angle = game.car.angle + (i * math.pi / (direction_num/2))
        # ë²½ê¹Œì§€ì˜ ê±°ë¦¬ë˜í•œ ê³„ì‚°í•˜ê¸° ìœ„í•´ 5ì”© ê³„ì‚° ì§„í–‰.
        for d in range(0, sensor_range, 5):
            x = int(game.car.x + math.cos(angle) * d)
            y = int(game.car.y + math.sin(angle) * d)
            if (x < 0 or y < 0 or 
                x >= game.track_mask.shape[1] or 
                y >= game.track_mask.shape[0] or
                game.track_mask[y, x] == 0):
                sensors.append(d)
                break
        else:
            sensors.append(sensor_range)
    return sensors


def get_data(game, standard_cp=None, dis_gap=None): 
    angle = game.car.angle
    cos_angle = (math.cos(angle) + 1) / 2
    sin_angle = (math.sin(angle) + 1) / 2
    speed = game.car.speed / game.car.max_speed
    vel_x = game.car.velocity_x / game.car.max_speed
    vel_y = game.car.velocity_y / game.car.max_speed
    sensors = [sensor / game.car.sensor_range for sensor in get_sensors(game)]
    
    if standard_cp is not None and dis_gap is not None and dis_gap > 0:
        # ì²´í¬í¬ì¸íŠ¸ê¹Œì§€ ê±°ë¦¬
        current_distance = math.dist([game.car.x, game.car.y], standard_cp)
        normalized_dist = min(current_distance / dis_gap, 1.5)
        is_drifting = 1.0 if game.car.is_drifting else 0.0
        # ì²´í¬í¬ì¸íŠ¸ ë°©í–¥ (ìƒëŒ€ ê°ë„)
        dx = standard_cp[0] - game.car.x
        dy = standard_cp[1] - game.car.y
        target_angle = math.atan2(dy, dx)
        relative_angle = target_angle - game.car.angle
        
        # -Ï€ ~ Ï€ ì •ê·œí™”
        while relative_angle > math.pi:
            relative_angle -= 2 * math.pi
        while relative_angle < -math.pi:
            relative_angle += 2 * math.pi
        
        # 0~1ë¡œ ì •ê·œí™”
        normalized_angle = (relative_angle + math.pi) / (2 * math.pi)
    else:
        normalized_dist = 1.0
        normalized_angle = 0.5
        is_drifting = 0.0
    
    car_features = [
        game.car.max_speed / 1000,              # ìµœëŒ€ì†ë„
        game.car.acceleration_force / 1000,     # ê°€ì†ë ¥
        game.car.brake_force,                  # ë¸Œë ˆì´í¬ íš¨ìœ¨ 
        game.car.base_friction,                # ë§ˆì°°ê³„ìˆ˜
        game.car.lateral_friction,       # ì¸¡ë©´ ë§ˆì°°
        game.car.turn_speed / 10,               # íšŒì „ì†ë„
        game.car.drift_lateral_friction,                # ë“œë¦¬í”„íŠ¸ ë§ˆì°°
        game.car.sensor_range / 1000            # ì„¼ì„œ ë²”ìœ„
    ]
    
    return sensors + [cos_angle, sin_angle, speed, vel_x, vel_y, normalized_dist, normalized_angle, is_drifting] + car_features


def execute_action_with_frame_skip(game, policy_net, action, ori_checkpoints, 
                                    current_segment, standard_cp, dis_gap, max_time):
    """
    Actionì„ ì§€ì •ëœ frame ìˆ˜ë§Œí¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜
    
    Args:
        game: RacingGame ì¸ìŠ¤í„´ìŠ¤ - ê²Œì„ ìƒíƒœ ë° ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜
        policy_net: DQN ì¸ìŠ¤í„´ìŠ¤ - ì •ì±… ë„¤íŠ¸ì›Œí¬ (ì•¡ì…˜ í”„ë ˆì„ ìˆ˜, ì‹¤ì œ ì•¡ì…˜ ë³€í™˜, ë¦¬ì›Œë“œ ê³„ì‚°)
        action: int - ì„ íƒëœ ì•¡ì…˜ ì¸ë±ìŠ¤ (0~5)
        ori_checkpoints: list - ì›ë³¸ ì²´í¬í¬ì¸íŠ¸ ë¦¬ìŠ¤íŠ¸ (ì²´í¬í¬ì¸íŠ¸ ë„ë‹¬ ì‹œ ë‹¤ìŒ ëª©í‘œ ê²°ì •ìš©)
        current_segment: int - í˜„ì¬ íŠ¸ë™ ì„¸ê·¸ë¨¼íŠ¸ ì¸ë±ìŠ¤ (ì²´í¬í¬ì¸íŠ¸ ê¸°ì¤€ìœ¼ë¡œ íŠ¸ë™ì„ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆˆ ê²ƒ)
        standard_cp: tuple - í˜„ì¬ ëª©í‘œ ì²´í¬í¬ì¸íŠ¸ ì¢Œí‘œ (x, y)
        dis_gap: float - í˜„ì¬ ìœ„ì¹˜ì—ì„œ ëª©í‘œ ì²´í¬í¬ì¸íŠ¸ê¹Œì§€ì˜ ê±°ë¦¬
        max_time: float - ì—í”¼ì†Œë“œ ìµœëŒ€ ì‹œê°„ (ì´ˆ ë‹¨ìœ„)
    
    Returns:
        dict - ì•¡ì…˜ ì‹¤í–‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    # ì•¡ì…˜ì— í•´ë‹¹í•˜ëŠ” í”„ë ˆì„ ìˆ˜ (ì•¡ì…˜ë³„ë¡œ ë‹¤ë¥¸ í”„ë ˆì„ ìˆ˜ ì ìš©)
    frames = policy_net.get_action_frames(action)
    # ì•¡ì…˜ ì¸ë±ìŠ¤ë¥¼ ì‹¤ì œ ê²Œì„ ì œì–´ ëª…ë ¹ìœ¼ë¡œ ë³€í™˜ (forward, left, right, brake ë“±)
    controls = policy_net.get_real_action(action)
    
    # ëˆ„ì  ë¦¬ì›Œë“œ (ëª¨ë“  í”„ë ˆì„ì˜ ë¦¬ì›Œë“œ í•©ê³„)
    total_reward = 0
    # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì—¬ë¶€ (ê³¨ ë„ë‹¬, ì¶©ëŒ, íƒ€ì„ì•„ì›ƒ ì¤‘ í•˜ë‚˜)
    done = False
    # ê³¨ ë„ë‹¬ ì—¬ë¶€
    is_goal = False
    # ì¶©ëŒ ë°œìƒ ì—¬ë¶€
    is_collision = False
    # íƒ€ì„ì•„ì›ƒ ë°œìƒ ì—¬ë¶€
    is_timeout = False
    # ë„ë‹¬í•œ ì„¸ê·¸ë¨¼íŠ¸ ì¸ë±ìŠ¤ (ì²´í¬í¬ì¸íŠ¸ ë„ë‹¬ ì‹œ ì„¤ì •, Noneì´ë©´ ë¯¸ë„ë‹¬)
    segment_reached = None
    
    # í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ ì¸ë±ìŠ¤ (ì²´í¬í¬ì¸íŠ¸ ë„ë‹¬ ì‹œ ì—…ë°ì´íŠ¸ë¨)
    curr_segment = current_segment
    # í˜„ì¬ ëª©í‘œ ì²´í¬í¬ì¸íŠ¸ ì¢Œí‘œ (ì²´í¬í¬ì¸íŠ¸ ë„ë‹¬ ì‹œ ë‹¤ìŒ ì²´í¬í¬ì¸íŠ¸ë¡œ ì—…ë°ì´íŠ¸)
    curr_standard_cp = standard_cp
    # í˜„ì¬ ìœ„ì¹˜ì—ì„œ ëª©í‘œ ì²´í¬í¬ì¸íŠ¸ê¹Œì§€ì˜ ê±°ë¦¬ (ë§¤ í”„ë ˆì„ ì—…ë°ì´íŠ¸)
    curr_dis_gap = dis_gap
    
    # ê° í”„ë ˆì„ì˜ ì†ë„ ê°’ì„ ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸ (í‰ê·  ì†ë„ ê³„ì‚°ìš©)
    speed_list = []
    
    # ì•¡ì…˜ì— ì§€ì •ëœ í”„ë ˆì„ ìˆ˜ë§Œí¼ ë°˜ë³µ ì‹¤í–‰
    for frame_idx in range(frames):
        # ì¶©ëŒì´ë‚˜ ê³¨ ë„ë‹¬ ì‹œ ì¦‰ì‹œ ì¢…ë£Œ
        if game.collision or game.goal_reached:
            break
        
        # ì´ì „ ê±°ë¦¬ ê³„ì‚° (ë¦¬ì›Œë“œ ê³„ì‚°ìš©)
        prev_distance = math.dist([game.car.x, game.car.y], 
                                   [curr_standard_cp[0], curr_standard_cp[1]])
            
        # ê²Œì„ í•œ í”„ë ˆì„ ì§„í–‰ (controlsì— ë”°ë¼ ì°¨ëŸ‰ ì œì–´)
        _, step_done, info = game.step(controls)
        # í˜„ì¬ ê²Œì„ ê²½ê³¼ ì‹œê°„ (ë°€ë¦¬ì´ˆ ë‹¨ìœ„)
        curr_time = game.current_time
        
        # í˜„ì¬ ì°¨ëŸ‰ ìœ„ì¹˜ì—ì„œ ëª©í‘œ ì²´í¬í¬ì¸íŠ¸ê¹Œì§€ì˜ ìœ í´ë¦¬ë“œ ê±°ë¦¬
        curr_distance = math.dist([game.car.x, game.car.y], 
                                   [curr_standard_cp[0], curr_standard_cp[1]])
        
        # í˜„ì¬ ê²Œì„ ìƒíƒœë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (ì„¼ì„œê°’, ê°ë„, ì†ë„ ë“±)
        frame_state = get_data(game, curr_standard_cp, curr_dis_gap)
        # ì •ê·œí™”ëœ ì†ë„ ê°’ ì €ì¥ (frame_state[14]ì€ ì •ê·œí™”ëœ ì†ë„)
        speed_list.append(frame_state[14])
        
        # íƒ€ì„ì•„ì›ƒ ì—¬ë¶€ í™•ì¸ (ê²½ê³¼ ì‹œê°„ì´ max_time ì´ˆë¥¼ ì´ˆê³¼í–ˆëŠ”ì§€)
        timeout = curr_time / 1000 > max_time
        
        # ì²´í¬í¬ì¸íŠ¸ ë³´ìƒ (ê¸°ë³¸ê°’ 0, ì²´í¬í¬ì¸íŠ¸ ë„ë‹¬ ì‹œ 300)
        cp_r = 0
        # ì²´í¬í¬ì¸íŠ¸ ë„ë‹¬ ì—¬ë¶€ í™•ì¸
        if len(game.checkpoints_reached) > 0:
            cp_r = 50  # ì²´í¬í¬ì¸íŠ¸ ë„ë‹¬ ë³´ìƒ
            # ë„ë‹¬í•œ ì²´í¬í¬ì¸íŠ¸ ì¸ë±ìŠ¤ (ê²Œì„ ë‚´ë¶€ ì²´í¬í¬ì¸íŠ¸ ë¦¬ìŠ¤íŠ¸ ê¸°ì¤€)
            cp_idx = game.checkpoints_reached.pop(0)
            # ë„ë‹¬í•œ ì²´í¬í¬ì¸íŠ¸ ì¢Œí‘œ
            reached_cp = game.checkpoints[cp_idx]
            # ë„ë‹¬í•œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì œê±°
            game.checkpoints.pop(cp_idx)
            # ì›ë³¸ ì²´í¬í¬ì¸íŠ¸ ë¦¬ìŠ¤íŠ¸ì—ì„œì˜ ì¸ë±ìŠ¤
            ori_cp_idx = ori_checkpoints.index(reached_cp)
            
            # ë„ë‹¬í•œ ì„¸ê·¸ë¨¼íŠ¸ ì¸ë±ìŠ¤ ì €ì¥ (í•™ìŠµ í†µê³„ìš©)
            segment_reached = curr_segment
            
            # ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ì—…ë°ì´íŠ¸ (ori_cp_idx + 1)
            curr_segment = ori_cp_idx + 1
            # ë‹¤ìŒ ëª©í‘œ ì²´í¬í¬ì¸íŠ¸ ì„¤ì • (ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ë©´ ê³¨ ìœ„ì¹˜ë¡œ)
            curr_standard_cp = ori_checkpoints[ori_cp_idx + 1] if ori_cp_idx < len(ori_checkpoints) - 1 else game.end_pos
            # ë„ë‹¬í•œ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë‹¤ìŒ ëª©í‘œê¹Œì§€ì˜ ê±°ë¦¬ ê³„ì‚°
            curr_dis_gap = math.dist([reached_cp[0], reached_cp[1]], 
                                      [curr_standard_cp[0], curr_standard_cp[1]])
        
        # ê²Œì„ ìŠ¤í… ì¢…ë£Œ ì—¬ë¶€ í™•ì¸
        if step_done:
            if game.goal_reached:
                is_goal = True
                done = True
            elif game.collision:
                is_collision = True
                done = True
        
        # íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬
        if timeout:
            is_timeout = True
            cp_r = -300  # íƒ€ì„ì•„ì›ƒ í˜ë„í‹°
            done = True
        
        # í˜„ì¬ í”„ë ˆì„ì˜ ë¦¬ì›Œë“œ ê³„ì‚°
        frame_reward = policy_net.get_frame_reward(
            frame_state, is_collision, is_goal, curr_distance,
            curr_time, max_time, cp_r, curr_dis_gap, action, game.car.max_speed, prev_distance
        )
        # ëˆ„ì  ë¦¬ì›Œë“œì— ì¶”ê°€
        total_reward += frame_reward
        
        # ì¢…ë£Œ ì¡°ê±´ ë§Œì¡± ì‹œ ë£¨í”„ íƒˆì¶œ
        if done:
            break
    
    # ì•¡ì…˜ ì‹¤í–‰ í›„ ìµœì¢… ìƒíƒœ ë²¡í„° (ë‹¤ìŒ ìƒíƒœë¡œ ì‚¬ìš©ë¨)
    next_state = get_data(game, curr_standard_cp, curr_dis_gap)
    
    return {
        'total_reward': total_reward,  # ì•¡ì…˜ ì‹¤í–‰ ì¤‘ ëˆ„ì ëœ ì´ ë¦¬ì›Œë“œ
        'next_state': next_state,  # ì•¡ì…˜ ì‹¤í–‰ í›„ì˜ ê²Œì„ ìƒíƒœ ë²¡í„° (ë‹¤ìŒ ìƒíƒœ)
        'done': done,  # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì—¬ë¶€ (ê³¨/ì¶©ëŒ/íƒ€ì„ì•„ì›ƒ)
        'current_segment': curr_segment,  # ì—…ë°ì´íŠ¸ëœ í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ ì¸ë±ìŠ¤
        'standard_cp': curr_standard_cp,  # ì—…ë°ì´íŠ¸ëœ ëª©í‘œ ì²´í¬í¬ì¸íŠ¸ ì¢Œí‘œ
        'dis_gap': curr_dis_gap,  # ì—…ë°ì´íŠ¸ëœ ëª©í‘œê¹Œì§€ì˜ ê±°ë¦¬
        'segment_reached': segment_reached,  # ë„ë‹¬í•œ ì„¸ê·¸ë¨¼íŠ¸ ì¸ë±ìŠ¤ (Noneì´ë©´ ë¯¸ë„ë‹¬)
        'is_goal': is_goal,  # ê³¨ ë„ë‹¬ ì—¬ë¶€
        'is_collision': is_collision,  # ì¶©ëŒ ë°œìƒ ì—¬ë¶€
        'is_timeout': is_timeout,  # íƒ€ì„ì•„ì›ƒ ë°œìƒ ì—¬ë¶€
        'curr_time': game.current_time,  # í˜„ì¬ ê²Œì„ ê²½ê³¼ ì‹œê°„ (ë°€ë¦¬ì´ˆ)
        'speed_list': speed_list  # ê° í”„ë ˆì„ì˜ ì •ê·œí™”ëœ ì†ë„ ê°’ ë¦¬ìŠ¤íŠ¸
    }


def evaluate_model(policy_net, game, ori_checkpoints, max_time, num_tests=10):
    """
    [ìƒˆë¡œìš´ í•¨ìˆ˜] ëª¨ë¸ í‰ê°€ - greedy policyë¡œ num_testsíšŒ í…ŒìŠ¤íŠ¸
    
    Returns:
        success: ëª¨ë“  í…ŒìŠ¤íŠ¸ ì„±ê³µ ì—¬ë¶€
        avg_speed: ì„±ê³µí•œ ê²½ìš° í‰ê·  ìŠ¤í”¼ë“œ, ì‹¤íŒ¨ ì‹œ None
        success_count: ì„±ê³µ íšŸìˆ˜
    """
    policy_net.model.eval()  # í‰ê°€ ëª¨ë“œ
    
    goal_times = []
    success_count = 0
    
    all_speed_list = []
    
    for test_idx in range(num_tests):
        # ê²Œì„ ë¦¬ì…‹
        game.reset()
        game.checkpoints = copy.deepcopy(ori_checkpoints)
        
        standard_cp = ori_checkpoints[0] if ori_checkpoints else game.end_pos
        dis_gap = math.dist([game.start_pos[0], game.start_pos[1]], 
                            [standard_cp[0], standard_cp[1]])
        current_segment = 0
        
        # ì—í”¼ì†Œë“œ ì‹¤í–‰ (greedy, epsilon=0)
        while True:
            state = get_data(game, standard_cp, dis_gap)
            
            # greedy=Trueë¡œ ìˆœìˆ˜ policyë§Œ ì‚¬ìš©
            _, action = policy_net.predict(state, greedy=True)
            
            frames = policy_net.get_action_frames(action)
            controls = policy_net.get_real_action(action)
            
            done = False
            is_goal = False
            is_collision = False
            
            for _ in range(frames):
                if game.collision or game.goal_reached:
                    break
                
                _, step_done, info = game.step(controls)
                curr_time = game.current_time
                
                all_speed_list.append(game.car.speed * 0.36)
                
                # ì²´í¬í¬ì¸íŠ¸ ì²˜ë¦¬
                if len(game.checkpoints_reached) > 0:
                    cp_idx = game.checkpoints_reached.pop(0)
                    reached_cp = game.checkpoints[cp_idx]
                    game.checkpoints.pop(cp_idx)
                    ori_cp_idx = ori_checkpoints.index(reached_cp)
                    current_segment = ori_cp_idx + 1
                    standard_cp = ori_checkpoints[ori_cp_idx + 1] if ori_cp_idx < len(ori_checkpoints) - 1 else game.end_pos
                    dis_gap = math.dist([reached_cp[0], reached_cp[1]], 
                                        [standard_cp[0], standard_cp[1]])
                
                if step_done:
                    if game.goal_reached:
                        is_goal = True
                        done = True
                    elif game.collision:
                        is_collision = True
                        done = True
                
                # íƒ€ì„ì•„ì›ƒ
                if curr_time / 1000 > 2:
                    done = True
                    return False, 1000, 0
                
                if done:
                    break
            
            if done:
                break
        
        # ê²°ê³¼ ê¸°ë¡
        if is_goal:
            success_count += 1
            goal_times.append(game.current_time / 1000)
    
    policy_net.model.train()  # ë‹¤ì‹œ í•™ìŠµ ëª¨ë“œë¡œ
    
    # ê²°ê³¼ ë°˜í™˜
    all_success = (success_count == num_tests)
    avg_speed = np.mean(all_speed_list)
    
    return all_success, avg_speed, success_count


def train_headless(max_episode=10000, output_size=6, replay_length=100000, 
                   target_update=2000, log_interval=100, save_interval=1000, batch_size = 512,
                   track_file="./track.json", 
                   car_json_path="./racing_car.json",
                   layer_num=3, 
                   max_size = 512, 
                   lr = 0.0001):
    """Headless í•™ìŠµ í•¨ìˆ˜ - ê°œì„ ëœ ëª¨ë¸ ì €ì¥ ê¸°ì¤€ ì ìš©"""
    
    pygame.init()
    
    game = RacingGame(track_file, car_json_path=car_json_path, headless=True)
    
    # ë¡œê·¸ íŒŒì¼ ì„¤ì •
    lr_str = str(lr).replace(".", "_")
    log_filename = f"DQN_withEval_log_lr_{lr_str}_L{layer_num}_S{max_size}.txt"
    log_file = open(log_filename, 'w', encoding='utf-8')
    
    def log_and_print(message):
        """ë¡œê·¸ íŒŒì¼ì— ì“°ê³  ë™ì‹œì— ì½˜ì†”ì—ë„ ì¶œë ¥"""
        timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
        log_message = f"[{timestamp}] {message}\n"
        log_file.write(log_message)
        log_file.flush()  # ì‹¤ì‹œê°„ ê¸°ë¡ì„ ìœ„í•´ flush
        print(message)
    
    log_and_print("=" * 60)
    log_and_print("ğŸš€ HEADLESS TRAINING MODE (DQN)")
    log_and_print(f"   Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}")
    log_and_print(f"   Save Criteria: 10/10 success + best avg time")
    log_and_print(f"   Layer: {layer_num}, MaxSize: {max_size}, LR: {lr}")
    log_and_print(f"   Log File: {log_filename}")
    log_and_print("=" * 60)
    
    ori_checkpoints = copy.deepcopy(game.checkpoints)
    num_segments = len(ori_checkpoints) + 1
    
    policy_net = DQN(input_size=28, output_size=output_size, 
                    replay_memory_length=replay_length, num_segments=num_segments,
                    lr=lr, layer_num=layer_num, max_size=max_size)
    target_net = DQN(input_size=28, output_size=output_size, 
                    replay_memory_length=0, num_segments=num_segments,
                    lr=lr, layer_num=layer_num, max_size=max_size)
    target_net.model.load_state_dict(policy_net.model.state_dict())
    target_net.model.eval()
    
    batch_size = batch_size
    
    episode = 0
    action_step = 0
    max_time = 10
    
    segment_counts = {i: 0 for i in range(num_segments)}
    segment_learned = {i: False for i in range(num_segments)}
    segment_threshold = 50
    current_segment = 0
    
    standard_cp = ori_checkpoints[0] if ori_checkpoints else game.end_pos
    dis_gap = math.dist([game.start_pos[0], game.start_pos[1]], 
                        [standard_cp[0], standard_cp[1]])
    
    goal_counts = 0
    # best_avg_speed = max_speed  # [ë³€ê²½] ìµœê³  í‰ê·  ì†ë„
    best_avg_speed = 0
    save_episode = 0
    episode_rewards = []
    current_episode_reward = 0
    
    start_time = time.time()
    last_log_time = start_time
    
    log_and_print(f"\nğŸ“Š Training started at {time.strftime('%H:%M:%S')}")
    log_and_print("-" * 60)
    
    saved_counts = 0
    
    all_speed_list = []
    
    while episode < max_episode:
        learned_until = -1
        for i in range(num_segments):
            if segment_learned[i]:
                learned_until = i
            else:
                break
        
        policy_net.update_epsilon(current_segment, learned_until)
        
        state = get_data(game, standard_cp, dis_gap)
        _, action = policy_net.predict(state)
        
        # ì•¡ì…˜ ì‹¤í–‰ í›„, ê° í”„ë ˆì„ ë³„ ì–»ì€ ê²°ê³¼ë“¤ì„ ì´í•©í•˜ëŠ” í•¨ìˆ˜.
        # execute_action_with_frame_skip() í•¨ìˆ˜ ë‚´ë¶€ ë³€ìˆ˜ ì„¤ëª…:
        #   - game: ê²Œì„ ì¸ìŠ¤í„´ìŠ¤ (ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ë° ìƒíƒœ ê´€ë¦¬)
        #   - policy_net: ì •ì±… ë„¤íŠ¸ì›Œí¬ (ì•¡ì…˜ í”„ë ˆì„ ìˆ˜, ì‹¤ì œ ì•¡ì…˜ ë³€í™˜, ë¦¬ì›Œë“œ ê³„ì‚°)
        #   - action: ì„ íƒëœ ì•¡ì…˜ ì¸ë±ìŠ¤ (0~5)
        #   - ori_checkpoints: ì›ë³¸ ì²´í¬í¬ì¸íŠ¸ ë¦¬ìŠ¤íŠ¸ (ì²´í¬í¬ì¸íŠ¸ ë„ë‹¬ ì‹œ ë‹¤ìŒ ëª©í‘œ ê²°ì •)
        #   - current_segment: í˜„ì¬ íŠ¸ë™ ì„¸ê·¸ë¨¼íŠ¸ ì¸ë±ìŠ¤
        #   - standard_cp: í˜„ì¬ ëª©í‘œ ì²´í¬í¬ì¸íŠ¸ ì¢Œí‘œ
        #   - dis_gap: í˜„ì¬ ìœ„ì¹˜ì—ì„œ ëª©í‘œ ì²´í¬í¬ì¸íŠ¸ê¹Œì§€ì˜ ê±°ë¦¬
        #   - max_time: ì—í”¼ì†Œë“œ ìµœëŒ€ ì‹œê°„ (ì´ˆ)
        result = execute_action_with_frame_skip(
            game, policy_net, action, ori_checkpoints,
            current_segment, standard_cp, dis_gap, max_time
        )
        
        # execute_action_with_frame_skip() ë°˜í™˜ê°’ ì‚¬ìš©:
        total_reward = result['total_reward']  # ì•¡ì…˜ ì‹¤í–‰ ì¤‘ ëˆ„ì ëœ ì´ ë¦¬ì›Œë“œ
        next_state = result['next_state']  # ì•¡ì…˜ ì‹¤í–‰ í›„ì˜ ê²Œì„ ìƒíƒœ ë²¡í„°
        done = result['done']  # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì—¬ë¶€ (ê³¨/ì¶©ëŒ/íƒ€ì„ì•„ì›ƒ)
        current_segment = result['current_segment']  # ì—…ë°ì´íŠ¸ëœ í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ ì¸ë±ìŠ¤ (ì²´í¬í¬ì¸íŠ¸ ë„ë‹¬ ì‹œ ì¦ê°€)
        standard_cp = result['standard_cp']  # ì—…ë°ì´íŠ¸ëœ ëª©í‘œ ì²´í¬í¬ì¸íŠ¸ ì¢Œí‘œ
        dis_gap = result['dis_gap']  # ì—…ë°ì´íŠ¸ëœ ëª©í‘œê¹Œì§€ì˜ ê±°ë¦¬
        all_speed_list += result['speed_list']  # ê° í”„ë ˆì„ì˜ ì •ê·œí™”ëœ ì†ë„ ê°’ ë¦¬ìŠ¤íŠ¸ (í‰ê·  ì†ë„ ê³„ì‚°ìš©)
        
        policy_net.add_memory([state, action, total_reward, next_state, done])
        current_episode_reward += total_reward
        action_step += 1
        
        if result['segment_reached'] is not None:
            reached_seg = result['segment_reached']
            segment_counts[reached_seg] += 1
            if segment_counts[reached_seg] >= segment_threshold and not segment_learned[reached_seg]:
                segment_learned[reached_seg] = True
                log_and_print(f"\nâ˜…â˜…â˜… Segment {reached_seg} í•™ìŠµ ì™„ë£Œ! (Episode: {episode}) â˜…â˜…â˜…\n")
        
        if result['is_goal']:
            goal_segment_idx = len(ori_checkpoints)
            segment_counts[goal_segment_idx] += 1
            if segment_counts[goal_segment_idx] >= segment_threshold and not segment_learned[goal_segment_idx]:
                segment_learned[goal_segment_idx] = True
                log_and_print(f"\nâ˜…â˜…â˜… GOAL segment í•™ìŠµ ì™„ë£Œ! (Episode: {episode}) â˜…â˜…â˜…\n")
        
        if done:
            episode += 1
            episode_rewards.append(current_episode_reward)
            
            if current_segment > learned_until:
                policy_net.decay_segment_epsilon(current_segment)
            
            if episode % log_interval == 0:
                elapsed = time.time() - last_log_time
                eps_per_sec = log_interval / elapsed if elapsed > 0 else 0
                avg_reward = np.mean(episode_rewards[-log_interval:]) if episode_rewards else 0
                log_msg = (f"[Ep {episode:5d}] Goal: {goal_counts:3d} | Seg: {current_segment} | "
                          f"Îµ: {policy_net.epsilon:.3f} | Reward: {avg_reward:8.1f} | "
                          f"Speed: {eps_per_sec:.1f} ep/s | Mem: {len(policy_net.replay_memory)} | "
                          f"Avg Speed: {np.mean(all_speed_list) * game.car.max_speed * 0.36: .2f} km/h")
                all_speed_list = []
                log_and_print(log_msg)
                last_log_time = time.time()
            
            if result['is_goal']:
                goal_counts += 1
                goal_time = result['curr_time'] / 1000
                log_and_print(f"  âœ“ GOAL! Episode {episode}, Time: {goal_time:.2f}s, Total Goals: {goal_counts}")
                
                # [ë³€ê²½] ëª¨ë¸ í‰ê°€ ë° ì €ì¥ ë¡œì§
                if goal_counts > 50:
                    saved_counts += 1
                    log_and_print(f"  ğŸ” Evaluating model (10 tests)...")
                    all_success, avg_speed, success_count = evaluate_model(
                        policy_net, game, ori_checkpoints, max_time, num_tests=10
                    )
                    
                    if all_success:
                        log_and_print(f"  âœ… Evaluation: {success_count}/10 success, Avg Time: {avg_speed:.3f}s")
                        
                        # if avg_speed < best_avg_speed:
                        if avg_speed > best_avg_speed: 
                            # ì´ì „ best ëª¨ë¸ ì‚­ì œ
                            lr_str = str(lr).replace(".", "_")
                            old_path = f'dqn_best_lr_{lr_str}_L{layer_num}_S{max_size}_E{save_episode}_T{str(round(best_avg_speed, 3)).replace(".", "_")}.pth'
                            if os.path.exists(old_path):
                                os.remove(old_path)
                            
                            # ìƒˆ best ëª¨ë¸ ì €ì¥
                            best_avg_speed = avg_speed
                            save_episode = episode
                            save_path = f'dqn_best_lr_{lr_str}_L{layer_num}_S{max_size}_E{save_episode}_T{str(round(best_avg_speed, 3)).replace(".", "_")}.pth'
                            torch.save(policy_net.model.state_dict(), save_path)
                            log_and_print(f"  ğŸ’¾ New best model saved! (Episode: {episode}, Avg Time: {best_avg_speed:.3f}s)")
                            saved_counts = 0
                    else:
                        log_and_print(f"  âŒ Evaluation: {success_count}/10 success - Not saved")
                    
                    # í‰ê°€ í›„ ê²Œì„ ìƒíƒœ ë³µêµ¬ (evaluate_modelì—ì„œ resetë¨)
                    game.reset()
                    game.checkpoints = copy.deepcopy(ori_checkpoints)
            
            # ê²Œì„ ë¦¬ì…‹
            game.reset()
            game.checkpoints = copy.deepcopy(ori_checkpoints)
            current_segment = 0
            standard_cp = ori_checkpoints[0] if ori_checkpoints else game.end_pos
            dis_gap = math.dist([game.start_pos[0], game.start_pos[1]], 
                                [standard_cp[0], standard_cp[1]])
            current_episode_reward = 0
        
        if action_step % 5 == 0 and len(policy_net.replay_memory) > batch_size:
            policy_net.train_step(batch_size, target_net)
        
        if action_step % target_update == 0:
            target_net.model.load_state_dict(policy_net.model.state_dict())
            target_net.model.eval()
        
        if saved_counts >= 500: 
            break
    
    total_time = time.time() - start_time
    log_and_print("\n" + "=" * 60)
    log_and_print("ğŸ TRAINING COMPLETED!")
    log_and_print(f"   Algorithm: DQN")
    log_and_print(f"   Total Episodes: {episode}")
    log_and_print(f"   Total Goals: {goal_counts}")
    log_and_print(f"   Best Avg Time: {best_avg_speed:.3f}s")
    log_and_print(f"   Training Time: {total_time/60:.1f} minutes")
    log_and_print(f"   Average Speed: {episode/total_time:.1f} episodes/second")
    log_and_print("=" * 60)
    
    log_file.close()
    pygame.quit()
    
    return policy_net

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument("--layer_num", type=int, default=4)
    parser.add_argument("--max_size", type=int, default=2048)
    parser.add_argument("--lr", type=float, default=0.0003)
    parser.add_argument("--batch_size", type=int, default=512)
    args = parser.parse_args()
    
    train_headless(
        max_episode = 300000,
        output_size = 6,
        replay_length = 100000,
        target_update = 2000,
        log_interval = 100,
        save_interval = 1000,
        layer_num = args.layer_num,
        max_size = args.max_size,
        lr = args.lr,
        batch_size = args.batch_size,
        car_json_path = "./racing_car.json",
        track_file = "./track.json"
    )